{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CS 334 - Algorithms of Machine Learning | Conrad Kennington*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Computer Science | Boise State University*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*11.02.2022 | Fall 2022*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aida Gomezbueno Berezo | aidagomezbuenobe@u.boisestate.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The decision tree algorithim is learning from Congressional Voting Records dataset, trying to predict party affiliation (Democrat/Republican) from 1984 Congressional Voting Records, while performing **Information Gain** splitting strategy.*\n",
    "\n",
    ">*This [data set](https://www.kaggle.com/datasets/devvret/congressional-voting-records) includes votes for each of the U.S. House of Representatives Congressmen on the 16 key votes identified by the CQA. The CQA lists nine different types of votes: voted for, paired for, and announced for (these three simplified to yea), voted against, paired against, and announced against (these three simplified to nay), voted present, voted present to avoid conflict of interest, and did not vote or otherwise make a position known (these three simplified to an unknown disposition).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download | Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Download\n",
    "house_votes = pd.read_csv(\"house-votes-84.csv\")\n",
    "\n",
    "#Set columns in order to be able to refer to them\n",
    "cols = ['Class Name', 'handicapped-infants', 'water-project-cost-sharing', 'adoption-of-the-budget-resolution', 'physician-fee-freeze', 'el-salvador-aid', 'religious-groups-in-schools', 'anti-satellite-test-ban', 'aid-to-nicaraguan-contras', 'mx-missile', 'immigration', 'synfuels-corporation-cutback', 'education-spending', 'superfund-right-to-sue', 'crime', 'duty-free-exports', 'export-administration-act-south-africa']\n",
    "house_votes.columns = cols\n",
    "feature_names = ['handicapped-infants','water-project-cost-sharing','adoption-of-the-budget-resolution','physician-fee-freeze','el-salvador-aid','religious-groups-in-schools','anti-satellite-test-ban','aid-to-nicaraguan-contras','mx-missile','immigration','synfuels-corporation-cutback','education-spending','superfund-right-to-sue','crime','duty-free-exports','export-administration-act-south-africa']\n",
    "label = ['Class-Name']\n",
    "\n",
    "#Get X and Y: features and label to train the model, then testing\n",
    "X = house_votes[feature_names].values\n",
    "Y = house_votes['Class Name'].values.reshape(-1,1)\n",
    "\n",
    "#Spliting dataset into training/testing sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, Y, test_size=0.30, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Oriented Fashion | Node & Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, left=None, right=None, threshold=None, info_gain=None, leaf_value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.threshold = threshold\n",
    "        self.info_gain = info_gain\n",
    "        self.leaf_value = leaf_value\n",
    "\n",
    "class DecisionTree():\n",
    "    \n",
    "    '''\n",
    "        Decision Tree constructor. Root: initialized as None, min_splits: to set min number of splits per split, max_depth: to set how many levels of splits do we want.\n",
    "    '''   \n",
    "    def __init__(self,min_splits=2, max_depth=2):\n",
    "        self.root = None\n",
    "        self.min_splits = min_splits\n",
    "        self.max_depth = max_depth\n",
    "     \n",
    "    '''\n",
    "        Build tree with depth 0.\n",
    "    '''\n",
    "    def grow_tree(self, data, depth=0):\n",
    "        X, Y = data[:,:-1],data[:,-1] \n",
    "        num_samples, num_features = np.shape(X)\n",
    "        #Consider bounds of tree: min_splits and max_depth\n",
    "        if num_samples >= self.min_splits and depth <= self.max_depth:\n",
    "            best_split = self.find_best_split(data, num_samples, num_features)\n",
    "            if best_split['info_gain'] > 0:\n",
    "                left_tree = self.grow_tree(best_split['subtree_left'], depth + 1)\n",
    "                right_tree = self.grow_tree(best_split['subtree_right'], depth + 1)\n",
    "                return Node(best_split['feature_index'], left_tree, right_tree, best_split['threshold'], best_split['info_gain'])\n",
    "        leaf_list = list(Y)\n",
    "        leaf_val = max(leaf_list, key=leaf_list.count)\n",
    "        return Node(leaf_value=leaf_val)\n",
    "    \n",
    "    '''\n",
    "        Run through all the features and its values in order to find the thresold that results in the best split and, thus, build the most accurate tree.\n",
    "    '''\n",
    "    def find_best_split(self, data, num_samples, num_features):\n",
    "        split_d = {}\n",
    "        best_gain = -1 \n",
    "        for feat in range(num_features):\n",
    "            vals = data[:feat]\n",
    "            unique_vals = np.unique(vals)\n",
    "            for threshold in unique_vals:\n",
    "                subtree_l, subtree_r = self.split(data, feat, threshold)\n",
    "                if len(subtree_l) > 0 and len(subtree_r) > 0:\n",
    "                    parent = data[:,-1]\n",
    "                    child_l = subtree_l[:,-1]\n",
    "                    child_r = subtree_r[:,-1]\n",
    "                    ig = self.information_gain(parent, child_l, child_r)\n",
    "                    if ig > best_gain: \n",
    "                        split_d['feature_index'] = feat\n",
    "                        split_d['threshold'] = threshold\n",
    "                        split_d['info_gain'] = ig\n",
    "                        split_d['subtree_left'] = subtree_l\n",
    "                        split_d['subtree_right'] = subtree_r\n",
    "                        best_gain = ig \n",
    "        return split_d\n",
    "    \n",
    "    '''\n",
    "        From the threshold previously found, split the data.\n",
    "    '''\n",
    "    def split(self, data, i, threshold):\n",
    "        trues = []\n",
    "        falses = []\n",
    "        for row in data:\n",
    "            if row[i] <= threshold:\n",
    "                trues.append(row)\n",
    "            else:\n",
    "                falses.append(row)\n",
    "        subtree_l = np.array(trues)\n",
    "        subtree_r = np.array(falses)\n",
    "        return subtree_l, subtree_r\n",
    "    \n",
    "    '''\n",
    "        From the entropy of both, parent and (average of) child nodes, Information Gain is computed to determine how good the splitting of nodes in a decision tree are, and thus, split (find_best_split, split).\n",
    "    '''\n",
    "    def information_gain(self, parent, child_l, child_r):\n",
    "        weight_l = len(child_l) / len(parent)\n",
    "        weight_r = len(child_r) / len(parent)        \n",
    "        children = (weight_l * self.entropy_k(child_l)) + (weight_r * self.entropy_k(child_r))\n",
    "        ig = self.entropy_k(parent) - children #parent_entropy - child_entropy    \n",
    "        return ig\n",
    "    \n",
    "    '''\n",
    "        Used by Information Gain as a theory metric to measure the impurity or uncertainty in a group of observations (either nodes). \n",
    "    '''\n",
    "    def entropy_k(self, parent):\n",
    "        labels = np.unique(parent)\n",
    "        entropy_k = 1\n",
    "        for label in labels: #democrat vs republican\n",
    "            num_rows_true = parent[parent == label]\n",
    "            p = len(num_rows_true) / len(parent)\n",
    "            entropy_k = entropy_k - p**2 #exponentiation  \n",
    "        return entropy_k\n",
    "    \n",
    "    '''\n",
    "        It builds the decision tree from the given training set (X, Y). \n",
    "    '''\n",
    "    def _fit(self, X, Y):\n",
    "        data = np.concatenate((X, Y), axis=1) \n",
    "        self.root = self.grow_tree(data)\n",
    "    \n",
    "    '''\n",
    "        It gathers all the predictions by calling get_predictions. Then, returning an array with all the results.\n",
    "    '''\n",
    "    def predict(self, X):\n",
    "        results = []\n",
    "        for row in X:\n",
    "            results.append(self.get_predictions(self.root, row))\n",
    "        return pd.array(results)\n",
    "    \n",
    "    '''\n",
    "        It gets the final predictions (results) of the decision tree model.\n",
    "        It recursevily runs through all the tree until it reaches a leaf node, then returning it.\n",
    "    '''\n",
    "    def get_predictions(self, tree, x):\n",
    "        if tree.leaf_value is not None:\n",
    "            return tree.leaf_value\n",
    "        feat_id = x[tree.feature_index]\n",
    "        if feat_id <= tree.threshold:\n",
    "            return self.get_predictions(tree.left, x)\n",
    "        return self.get_predictions(tree.right, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing | Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.65% \n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(1000, 5)\n",
    "tree._fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(\"Accuracy: %.2f%% \" % (accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.13% \n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(100, 5)\n",
    "tree._fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "print(\"Accuracy: %.2f%% \" % (accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.89% \n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(2, 5)\n",
    "tree._fit(X_train, y_train)\n",
    "#tree.print_results()\n",
    "y_pred = tree.predict(X_test)\n",
    "print(\"Accuracy: %.2f%% \" % (accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "\t\t [81  5]\n",
      "\t\t [ 3 42]\n",
      "Accuracy: 93.89%\n",
      "Precision: 94.19%\n",
      "Recall/Sensitivity: 65.85%\n",
      "F1-Score: 77.51%\n",
      "Error rate: 6.11%\n",
      "Specific number of error:  8\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "tp = cf[0][0]\n",
    "fp = cf[0][1]\n",
    "fn = cf[1][0]\n",
    "tn = cf[1][1]\n",
    "#Accuracy\n",
    "acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "#Precision\n",
    "prec = tp/(tp+fp)\n",
    "#Recall/Sensitivity\n",
    "recall = tp/(tp+tn)\n",
    "#F1 score\n",
    "f1 = 2*((prec*recall)/(prec+recall))\n",
    "#Error rate = 1 - accuracy\n",
    "error_rate = 1-acc\n",
    "n_errors = fn+fp\n",
    "\n",
    "#Print results\n",
    "print(\"Confusion matrix: \")\n",
    "for i in cf:\n",
    "    print(\"\\t\\t\", i)\n",
    "print(\"Accuracy: %.2f%%\" % (acc*100))\n",
    "print(\"Precision: %.2f%%\" % (prec*100))\n",
    "print(\"Recall/Sensitivity: %.2f%%\" % (recall*100))\n",
    "print(\"F1-Score: %.2f%%\" % (f1*100))\n",
    "print(\"Error rate: %.2f%%\" % (error_rate*100))\n",
    "print(\"Specific number of error: \", n_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
